{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smaliyu/AfriNLP/blob/main/yoruba_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "try:\n",
        "  import wandb\n",
        "except:\n",
        "  !pip install -q wandb\n",
        "  import wandb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (XLMRobertaTokenizer, XLMRobertaForSequenceClassification,\n",
        "                          BertTokenizer, BertForSequenceClassification,\n",
        "                          AutoTokenizer, AutoModelForSequenceClassification)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm.auto import tqdm\n",
        "import os\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-09T16:09:07.824268Z",
          "iopub.execute_input": "2024-02-09T16:09:07.824654Z",
          "iopub.status.idle": "2024-02-09T16:09:07.832312Z",
          "shell.execute_reply.started": "2024-02-09T16:09:07.824626Z",
          "shell.execute_reply": "2024-02-09T16:09:07.831434Z"
        },
        "trusted": true,
        "id": "SqbFTH4mK5v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-09T14:12:28.877497Z",
          "iopub.execute_input": "2024-02-09T14:12:28.877897Z",
          "iopub.status.idle": "2024-02-09T14:12:33.172912Z",
          "shell.execute_reply.started": "2024-02-09T14:12:28.877868Z",
          "shell.execute_reply": "2024-02-09T14:12:33.171944Z"
        },
        "trusted": true,
        "id": "njTqhEidK5v8",
        "outputId": "21e3fee4-b968-4d61-90ae-97371ff59db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········································\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters in a config dictionary\n",
        "config = dict(\n",
        "    epochs=3,\n",
        "    batch_size=16,\n",
        "    learning_rate=5e-5,\n",
        "    dataset=\"Yoruba Tweets\",\n",
        "    architecture=\"Transformer-based\",\n",
        "    num_labels=None,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-09T16:09:10.139463Z",
          "iopub.execute_input": "2024-02-09T16:09:10.139863Z",
          "iopub.status.idle": "2024-02-09T16:09:10.144834Z",
          "shell.execute_reply.started": "2024-02-09T16:09:10.139836Z",
          "shell.execute_reply": "2024-02-09T16:09:10.143848Z"
        },
        "trusted": true,
        "id": "W-HnkvfdK5v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Load and preprocess datasets\n",
        "def load_and_preprocess_data(language):\n",
        "    train_data = pd.read_csv(f'https://raw.githubusercontent.com/smaliyu/AfriNLP/main/datasets/{language}/{language}_train.csv')\n",
        "    test_data = pd.read_csv(f'https://raw.githubusercontent.com/smaliyu/AfriNLP/main/datasets/{language}/{language}_test.csv')\n",
        "    train_texts = train_data['tweet'].tolist()\n",
        "    train_labels = label_encoder.fit_transform(train_data['label'].tolist())\n",
        "    test_texts = test_data['tweet'].tolist()\n",
        "    test_labels = label_encoder.transform(test_data['label'].tolist())\n",
        "    return train_texts, train_labels, test_texts, test_labels\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(texts, add_special_tokens=True, max_length=max_length,\n",
        "                     padding='max_length', return_attention_mask=True, truncation=True)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "def train(model, train_loader, optimizer, device, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        wandb.log({\"epoch\": epoch, \"train_loss\": avg_train_loss})\n",
        "\n",
        "# Evaluation Loop\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "    return predictions\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-09T16:09:13.336844Z",
          "iopub.execute_input": "2024-02-09T16:09:13.337219Z",
          "iopub.status.idle": "2024-02-09T16:09:13.353646Z",
          "shell.execute_reply.started": "2024-02-09T16:09:13.337190Z",
          "shell.execute_reply": "2024-02-09T16:09:13.352705Z"
        },
        "trusted": true,
        "id": "UQTlgtXUK5v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models and languages\n",
        "models = [\"xlm-roberta-base\", \"bert-base-multilingual-cased\",\"morit/XLM-T-full-xnli\",\"Davlan/naija-twitter-sentiment-afriberta-large\"]\n",
        "languages = [\"yoruba\"]\n",
        "results = []\n",
        "\n",
        "# Loop over models and languages\n",
        "for model_name in models:\n",
        "    for language in languages:\n",
        "        # Define hyperparameters in a config dictionary\n",
        "        config = dict(\n",
        "            epochs=3,\n",
        "            batch_size=16,\n",
        "            learning_rate=5e-5,\n",
        "            dataset=\"Igbo Tweets\",\n",
        "            architecture=\"Transformer-based\",\n",
        "            num_labels=None,)\n",
        "\n",
        "\n",
        "        run_name = f\"{model_name}--{language}\"\n",
        "        wandb.init(project=\"hate-models\", config=config, name=run_name)\n",
        "        config = wandb.config\n",
        "\n",
        "        # Update tokenizer in config based on current model\n",
        "        wandb.config.update({\"tokenizer\": model_name}, allow_val_change=True)\n",
        "        print(f\"Training {model_name} on {language} dataset\")\n",
        "        train_texts, train_labels, test_texts, test_labels = load_and_preprocess_data(language)\n",
        "\n",
        "        # Update number of labels in config based on current dataset\n",
        "        wandb.config.update({\"num_labels\": len(np.unique(train_labels))}, allow_val_change=True)\n",
        "\n",
        "        # Tokenizer and Model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=wandb.config.num_labels)\n",
        "\n",
        "        # Tokenize data\n",
        "        train_encodings = tokenize(train_texts, tokenizer)\n",
        "        test_encodings = tokenize(test_texts, tokenizer)\n",
        "\n",
        "        # Create Datasets and DataLoaders\n",
        "        train_dataset = TextDataset(train_encodings, train_labels)\n",
        "        test_dataset = TextDataset(test_encodings, test_labels)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
        "\n",
        "        # Define optimizer\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "        # Device setup\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model.to(device)\n",
        "\n",
        "        # Add WandB watch\n",
        "        wandb.watch(model, log='all', log_freq=10)\n",
        "\n",
        "        # Train\n",
        "        train(model, train_loader, optimizer, device, config.epochs)\n",
        "        predictions = evaluate(model, test_loader, device)\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        test_accuracy = accuracy_score(test_labels, predictions)\n",
        "        test_precision = precision_score(test_labels, predictions, average='macro')\n",
        "        test_recall = recall_score(test_labels, predictions, average='macro')\n",
        "        test_f1 = f1_score(test_labels, predictions, average='macro')\n",
        "\n",
        "        # Store results in dictionary\n",
        "        result = {\n",
        "            \"Model\": model_name,\n",
        "            \"Language\": language,\n",
        "            \"Accuracy\": test_accuracy,\n",
        "            \"Precision\": test_precision,\n",
        "            \"Recall\": test_recall,\n",
        "            \"F1-score\": test_f1\n",
        "        }\n",
        "\n",
        "        # Append results to list\n",
        "        results.append(result)\n",
        "\n",
        "        # Log results\n",
        "        wandb.log({\"test_accuracy\": test_accuracy})\n",
        "\n",
        "        print(f\"Test Accuracy for {model_name} on {language}: {test_accuracy}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv(\"igbo_model_evaluation_results.csv\", index=False)\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-09T16:21:25.258307Z",
          "iopub.execute_input": "2024-02-09T16:21:25.258757Z",
          "iopub.status.idle": "2024-02-09T17:09:42.914749Z",
          "shell.execute_reply.started": "2024-02-09T16:21:25.258728Z",
          "shell.execute_reply": "2024-02-09T17:09:42.913929Z"
        },
        "trusted": true,
        "id": "8OiycxCDK5v_",
        "outputId": "620b492b-6678-45fc-a6da-9f3208c01ffd",
        "colab": {
          "referenced_widgets": [
            "",
            "de022a6ead684355899086c8405db05d",
            "3dd0611d0fc840f7854c75c5ad969f16",
            "88e9174d7c974208861b16d6d85b3c5f",
            "df0db780adb44686a397db4d366983fc",
            "844be43f1d974bff9cbd9303028b6408",
            "b8debfdc61ef473c984469d0c6ba1701",
            "1591b79d2b8348098d3d485176a57de4",
            "b8dff2981fd4485a9f8faf73a8804c14",
            "fb8dbb73da2b4f3ea313351d3b0dc547",
            "704c8c57b60a44abaf19459b5844b7a9",
            "3feefd3db4c74597a0d103849197f40e",
            "794c04cb5fbf4c72ad3c08e193d52cca",
            "95ed85cfd7f646b0915b2d601914ee2e",
            "cada18fe7ac5429d913a7b8dc0dd64b7",
            "7cdb62686055439581a6813f5d6b3669"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:9fwx3kjb) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.048 MB of 0.048 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">xlm-roberta-base--yoruba</strong> at: <a href='https://wandb.ai/lukmanaj/hate-models/runs/9fwx3kjb' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/9fwx3kjb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240209_161938-9fwx3kjb/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:9fwx3kjb). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240209_162125-faeu8757</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/lukmanaj/hate-models/runs/faeu8757' target=\"_blank\">xlm-roberta-base--yoruba</a></strong> to <a href='https://wandb.ai/lukmanaj/hate-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/lukmanaj/hate-models' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/lukmanaj/hate-models/runs/faeu8757' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/faeu8757</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Training xlm-roberta-base on yoruba dataset\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de022a6ead684355899086c8405db05d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Test Accuracy for xlm-roberta-base on yoruba: 0.834685598377282\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:faeu8757) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_accuracy</td><td>0.83469</td></tr><tr><td>train_loss</td><td>0.46407</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">xlm-roberta-base--yoruba</strong> at: <a href='https://wandb.ai/lukmanaj/hate-models/runs/faeu8757' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/faeu8757</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240209_162125-faeu8757/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:faeu8757). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240209_163602-lzm77syh</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/lukmanaj/hate-models/runs/lzm77syh' target=\"_blank\">bert-base-multilingual-cased--yoruba</a></strong> to <a href='https://wandb.ai/lukmanaj/hate-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/lukmanaj/hate-models' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/lukmanaj/hate-models/runs/lzm77syh' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/lzm77syh</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Training bert-base-multilingual-cased on yoruba dataset\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dd0611d0fc840f7854c75c5ad969f16"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Test Accuracy for bert-base-multilingual-cased on yoruba: 0.8407707910750507\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:lzm77syh) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_accuracy</td><td>0.84077</td></tr><tr><td>train_loss</td><td>0.42478</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">bert-base-multilingual-cased--yoruba</strong> at: <a href='https://wandb.ai/lukmanaj/hate-models/runs/lzm77syh' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/lzm77syh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240209_163602-lzm77syh/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:lzm77syh). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240209_164636-w17sq7uy</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/lukmanaj/hate-models/runs/w17sq7uy' target=\"_blank\">morit/XLM-T-full-xnli--yoruba</a></strong> to <a href='https://wandb.ai/lukmanaj/hate-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/lukmanaj/hate-models' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/lukmanaj/hate-models/runs/w17sq7uy' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/w17sq7uy</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Training morit/XLM-T-full-xnli on yoruba dataset\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/524 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88e9174d7c974208861b16d6d85b3c5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df0db780adb44686a397db4d366983fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "844be43f1d974bff9cbd9303028b6408"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8debfdc61ef473c984469d0c6ba1701"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1591b79d2b8348098d3d485176a57de4"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8dff2981fd4485a9f8faf73a8804c14"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Test Accuracy for morit/XLM-T-full-xnli on yoruba: 0.8468559837728195\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:w17sq7uy) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_accuracy</td><td>0.84686</td></tr><tr><td>train_loss</td><td>0.35718</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">morit/XLM-T-full-xnli--yoruba</strong> at: <a href='https://wandb.ai/lukmanaj/hate-models/runs/w17sq7uy' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/w17sq7uy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240209_164636-w17sq7uy/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Successfully finished last run (ID:w17sq7uy). Initializing new run:<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240209_170131-vchn5ekd</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/lukmanaj/hate-models/runs/vchn5ekd' target=\"_blank\">Davlan/naija-twitter-sentiment-afriberta-large--yoruba</a></strong> to <a href='https://wandb.ai/lukmanaj/hate-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/lukmanaj/hate-models' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/lukmanaj/hate-models/runs/vchn5ekd' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/vchn5ekd</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Training Davlan/naija-twitter-sentiment-afriberta-large on yoruba dataset\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/429 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb8dbb73da2b4f3ea313351d3b0dc547"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/1.55M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "704c8c57b60a44abaf19459b5844b7a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3feefd3db4c74597a0d103849197f40e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "794c04cb5fbf4c72ad3c08e193d52cca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95ed85cfd7f646b0915b2d601914ee2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/503M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cada18fe7ac5429d913a7b8dc0dd64b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cdb62686055439581a6813f5d6b3669"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Test Accuracy for Davlan/naija-twitter-sentiment-afriberta-large on yoruba: 0.8580121703853956\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.659 MB of 0.659 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_accuracy</td><td>0.85801</td></tr><tr><td>train_loss</td><td>0.17106</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">Davlan/naija-twitter-sentiment-afriberta-large--yoruba</strong> at: <a href='https://wandb.ai/lukmanaj/hate-models/runs/vchn5ekd' target=\"_blank\">https://wandb.ai/lukmanaj/hate-models/runs/vchn5ekd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240209_170131-vchn5ekd/logs</code>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5-1pFdOK5wC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}